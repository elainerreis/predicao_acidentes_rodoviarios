{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "671d370d",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "\n",
    "Este notebook tem como objetivo desenvolver e avaliar modelos de classificação\n",
    "para prever a gravidade dos acidentes de trânsito, utilizando dados tratados\n",
    "da Polícia Rodoviária Federal (PRF) e Ministerio do transporte.\n",
    "\n",
    "O foco do estudo é identificar acidentes classificados como graves\n",
    "(feridos graves ou óbitos), a partir de variáveis relacionadas ao contexto,\n",
    "local e características do acidente.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "68b40d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd5359",
   "metadata": {},
   "source": [
    "# Carregamento e inspeção dos dados\n",
    "\n",
    "Os dados utilizados neste estudo foram previamente tratados e armazenados em formato CSV.\n",
    "Nesta etapa, o dataset é carregado e analisado quanto à sua estrutura,\n",
    "tipos de dados e quantidade de registros.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df3cfa11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/tratados/amostra_19-21.csv', sep= ',', encoding=\"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbb534c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 114536 entries, 0 to 114535\n",
      "Data columns (total 16 columns):\n",
      " #   Column                  Non-Null Count   Dtype  \n",
      "---  ------                  --------------   -----  \n",
      " 0   uf                      114536 non-null  object \n",
      " 1   br                      114297 non-null  float64\n",
      " 2   km                      114297 non-null  object \n",
      " 3   municipio               114536 non-null  object \n",
      " 4   dia_semana              114536 non-null  object \n",
      " 5   fase_dia                114536 non-null  object \n",
      " 6   sentido_via             114536 non-null  object \n",
      " 7   condicao_metereologica  114536 non-null  object \n",
      " 8   tipo_pista              114536 non-null  object \n",
      " 9   tracado_via             114536 non-null  object \n",
      " 10  uso_solo                114536 non-null  object \n",
      " 11  tipo_veiculo            110217 non-null  object \n",
      " 12  data_inversa            114536 non-null  object \n",
      " 13  ano                     114536 non-null  int64  \n",
      " 14  frota                   113260 non-null  float64\n",
      " 15  gravidade               114536 non-null  int64  \n",
      "dtypes: float64(2), int64(2), object(12)\n",
      "memory usage: 14.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0173982",
   "metadata": {},
   "source": [
    "# Modelo 1 – Regressão Logística\n",
    "\n",
    "A Regressão Logística foi utilizada como modelo base por ser amplamente aplicada\n",
    "em problemas de classificação binária e por permitir boa interpretabilidade.\n",
    "\n",
    "O uso do parâmetro `class_weight='balanced'` foi necessário devido ao\n",
    "desbalanceamento das classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89fa0158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "Melhores parâmetros:\n",
      "{'clf__C': 0.1, 'clf__class_weight': 'balanced', 'clf__penalty': 'l2', 'clf__solver': 'liblinear'}\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.69      0.79     19305\n",
      "           1       0.30      0.71      0.42      3603\n",
      "\n",
      "    accuracy                           0.70     22908\n",
      "   macro avg       0.61      0.70      0.61     22908\n",
      "weighted avg       0.83      0.70      0.73     22908\n",
      "\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[13368  5937]\n",
      " [ 1046  2557]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Separação das variáveis (exemplo)\n",
    "\n",
    "X = df.drop('gravidade', axis=1)\n",
    "y = df['gravidade']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 3. Definir colunas numéricas e categóricas\n",
    "\n",
    "numeric_features = ['br', 'frota']\n",
    "categorical_features = [\n",
    "    'uf', 'km', 'municipio', 'dia_semana', 'fase_dia',\n",
    "    'sentido_via', 'condicao_metereologica', 'tipo_pista',\n",
    "    'tracado_via', 'uso_solo', 'tipo_veiculo']\n",
    "\n",
    "# 4. Pré-processamento\n",
    "\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 5. Pipeline final\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocess', preprocess),\n",
    "    ('clf', LogisticRegression(max_iter=300))\n",
    "])\n",
    "\n",
    "# 6. Grade de hiperparâmetros\n",
    "\n",
    "param_grid = {\n",
    "    'clf__C': [0.01, 0.1, 1, 10],\n",
    "    'clf__solver': ['liblinear', 'saga'],\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    'clf__class_weight': [None, 'balanced']\n",
    "}\n",
    "\n",
    "# 7. GridSearchCV\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1',      \n",
    "    cv=3,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Treino\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# 8. Resultados\n",
    "\n",
    "print(\"Melhores parâmetros:\")\n",
    "print(grid.best_params_)\n",
    "\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nMatriz de Confusão:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f9259a",
   "metadata": {},
   "source": [
    "Recall da classe grave: ≈ 0,71\n",
    "\n",
    "F1-score da classe grave: ≈ 0,42\n",
    "\n",
    "Accuracy: ≈ 0,70\n",
    "\n",
    "O modelo apresenta boa capacidade de identificar acidentes graves, mas com baixa precisão, indicando um número elevado de falsos positivos. Ainda assim, é um baseline adequado e coerente com o problema."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80145f5",
   "metadata": {},
   "source": [
    "# Modelo 2 – XGBoost\n",
    "\n",
    "O XGBoost é um algoritmo baseado em árvores de decisão e boosting,\n",
    "sendo conhecido por seu bom desempenho em problemas com dados tabulares\n",
    "e relações não lineares.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b316837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scale_pos_weight = 5.3586398334489935\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Elaine\\Desktop\\TCC_acidentes\\venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [22:09:54] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Melhores parâmetros encontrados:\n",
      "{'clf__colsample_bytree': 0.7, 'clf__learning_rate': 0.1, 'clf__max_depth': 8, 'clf__n_estimators': 500, 'clf__subsample': 0.8}\n",
      "\n",
      "Relatório de Classificação:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.73      0.82     19305\n",
      "           1       0.33      0.73      0.46      3603\n",
      "\n",
      "    accuracy                           0.73     22908\n",
      "   macro avg       0.63      0.73      0.64     22908\n",
      "weighted avg       0.84      0.73      0.76     22908\n",
      "\n",
      "\n",
      "Matriz de Confusão:\n",
      "[[14026  5279]\n",
      " [  982  2621]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "X = df.drop('gravidade', axis=1)\n",
    "y = df['gravidade']\n",
    "\n",
    "# Separar por tipo\n",
    "categorical_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "\n",
    "# 2. Pré-processamento\n",
    "\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=True), categorical_cols),\n",
    "        ('num', 'passthrough', numeric_cols)\n",
    "    ]\n",
    ")\n",
    "# 3. Divisão treino/teste\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "\n",
    "# 4. XGBOOST — com parâmetros básicos\n",
    "\n",
    "\n",
    "# Cálculo do scale_pos_weight (importante!)\n",
    "ratio = y_train.value_counts()[0] / y_train.value_counts()[1]\n",
    "print(\"scale_pos_weight =\", ratio)\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric='logloss',\n",
    "    use_label_encoder=False,\n",
    "    tree_method='hist',      \n",
    "    scale_pos_weight=ratio,   # Ajuste para desbalanceamento\n",
    ")\n",
    "\n",
    "\n",
    "# 5. Pipeline\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocess', preprocess),\n",
    "    ('clf', xgb)\n",
    "])\n",
    "\n",
    "# 6. Grade de hiperparâmetros\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'clf__n_estimators': [200, 500],\n",
    "    'clf__max_depth': [4, 6, 8],\n",
    "    'clf__learning_rate': [0.01, 0.1],\n",
    "    'clf__subsample': [0.8, 1.0],\n",
    "    'clf__colsample_bytree': [0.7, 1.0]\n",
    "}\n",
    "\n",
    "\n",
    "# 7. Grid Search\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='f1_macro',\n",
    "    cv=3,\n",
    "    verbose=2,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# 8. Avaliação do Modelo\n",
    "\n",
    "\n",
    "print(\"\\nMelhores parâmetros encontrados:\")\n",
    "print(grid.best_params_)\n",
    "\n",
    "y_pred = grid.predict(X_test)\n",
    "\n",
    "print(\"\\nRelatório de Classificação:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(\"\\nMatriz de Confusão:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall da classe grave: ≈ 0,73\n",
    "\n",
    "F1-score da classe grave: ≈ 0,46\n",
    "\n",
    "Accuracy: ≈ 0,73\n",
    "\n",
    "O XGBoost supera a Regressão Logística em todas as métricas relevantes. O uso de scale_pos_weight mostrou-se fundamental para lidar com o desbalanceamento, resultando em melhor equilíbrio entre identificação de casos graves e controle de erros."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1d8672",
   "metadata": {},
   "source": [
    "# StratifiedKFold + Regressão Logística"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5448db07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numéricas: 4\n",
      "Categóricas: 10\n",
      "\n",
      "===== TREINANDO LOGISTIC REGRESSION =====\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "\n",
      "=== MELHORES PARÂMETROS (LR) ===\n",
      "{'clf__solver': 'lbfgs', 'clf__penalty': 'l2', 'clf__class_weight': 'balanced', 'clf__C': np.float64(0.12742749857031335)}\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO (LR) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.69      0.79     19305\n",
      "           1       0.30      0.70      0.42      3603\n",
      "\n",
      "    accuracy                           0.69     22908\n",
      "   macro avg       0.61      0.69      0.60     22908\n",
      "weighted avg       0.83      0.69      0.73     22908\n",
      "\n",
      "\n",
      "=== MATRIZ DE CONFUSÃO (LR) ===\n",
      "[[13266  6039]\n",
      " [ 1072  2531]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 2. Identificar tipos de colunas\n",
    "\n",
    "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "cat_cols = X.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "print(\"Numéricas:\", len(num_cols))\n",
    "print(\"Categóricas:\", len(cat_cols))\n",
    "\n",
    "# 3. Preprocessamento: imputação + encoding + scaling\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "            (\"scaler\", StandardScaler())\n",
    "        ]), num_cols),\n",
    "\n",
    "        (\"cat\", Pipeline([\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
    "        ]), cat_cols)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 4. Train/test split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# 5. StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# 6. PIPELINE + LOGISTIC REGRESSION\n",
    "\n",
    "pipe_lr = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"clf\", LogisticRegression(max_iter=500))\n",
    "])\n",
    "\n",
    "param_lr = {\n",
    "    \"clf__C\": np.logspace(-3, 2, 20),\n",
    "    \"clf__penalty\": [\"l2\"],\n",
    "    \"clf__class_weight\": [None, \"balanced\"],\n",
    "    \"clf__solver\": [\"liblinear\", \"lbfgs\"]\n",
    "}\n",
    "\n",
    "search_lr = RandomizedSearchCV(\n",
    "    pipe_lr,\n",
    "    param_distributions=param_lr,\n",
    "    n_iter=30,\n",
    "    scoring=\"f1\",\n",
    "    cv=skf,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\n===== TREINANDO LOGISTIC REGRESSION =====\")\n",
    "search_lr.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n=== MELHORES PARÂMETROS (LR) ===\")\n",
    "print(search_lr.best_params_)\n",
    "\n",
    "best_lr = search_lr.best_estimator_\n",
    "\n",
    "y_pred_lr = best_lr.predict(X_test)\n",
    "\n",
    "print(\"\\n=== RELATÓRIO DE CLASSIFICAÇÃO (LR) ===\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "print(\"\\n=== MATRIZ DE CONFUSÃO (LR) ===\")\n",
    "print(confusion_matrix(y_test, y_pred_lr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fb117e",
   "metadata": {},
   "source": [
    "Recall da classe grave: ≈ 0,70\n",
    "\n",
    "F1-score da classe grave: ≈ 0,42\n",
    "\n",
    "Accuracy: ≈ 0,69\n",
    "\n",
    "A validação cruzada estratificada confirma que o desempenho da Regressão Logística é estável, porém limitado. O modelo não apresenta ganhos relevantes em relação à abordagem simples treino–teste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cf6874",
   "metadata": {},
   "source": [
    "# StratifiedKFold + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4d292",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TREINANDO XGBOOST =====\n",
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Elaine\\Desktop\\TCC_acidentes\\venv\\Lib\\site-packages\\xgboost\\training.py:199: UserWarning: [00:17:04] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:790: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== MELHORES PARÂMETROS (XGBoost) ===\n",
      "{'clf__subsample': 1.0, 'clf__n_estimators': 500, 'clf__max_depth': 8, 'clf__learning_rate': 0.1, 'clf__colsample_bytree': 0.8}\n",
      "\n",
      "=== RELATÓRIO DE CLASSIFICAÇÃO (XGB) ===\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.73      0.82     19305\n",
      "           1       0.33      0.72      0.45      3603\n",
      "\n",
      "    accuracy                           0.73     22908\n",
      "   macro avg       0.63      0.72      0.63     22908\n",
      "weighted avg       0.84      0.73      0.76     22908\n",
      "\n",
      "\n",
      "=== MATRIZ DE CONFUSÃO (XGB) ===\n",
      "[[14017  5288]\n",
      " [ 1006  2597]]\n"
     ]
    }
   ],
   "source": [
    "# 7. PIPELINE + XGBOOST\n",
    "\n",
    "pipe_xgb = Pipeline([\n",
    "    (\"prep\", preprocess),\n",
    "    (\"clf\", XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        scale_pos_weight=y_train.value_counts()[0] / y_train.value_counts()[1],\n",
    "        eval_metric=\"logloss\",\n",
    "        use_label_encoder=False,\n",
    "        tree_method=\"hist\"\n",
    "    ))\n",
    "])\n",
    "\n",
    "param_xgb = {\n",
    "    \"clf__n_estimators\": [200, 300, 500],\n",
    "    \"clf__max_depth\": [4, 6, 8],\n",
    "    \"clf__learning_rate\": [0.01, 0.05, 0.1],\n",
    "    \"clf__subsample\": [0.6, 0.8, 1.0],\n",
    "    \"clf__colsample_bytree\": [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "search_xgb = RandomizedSearchCV(\n",
    "    pipe_xgb,\n",
    "    param_distributions=param_xgb,\n",
    "    n_iter=30,\n",
    "    scoring=\"f1\",\n",
    "    cv=skf,\n",
    "    verbose=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(\"\\n===== TREINANDO XGBOOST =====\")\n",
    "search_xgb.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\n=== MELHORES PARÂMETROS (XGBoost) ===\")\n",
    "print(search_xgb.best_params_)\n",
    "\n",
    "best_xgb = search_xgb.best_estimator_\n",
    "\n",
    "y_pred_xgb = best_xgb.predict(X_test)\n",
    "\n",
    "print(\"\\n=== RELATÓRIO DE CLASSIFICAÇÃO (XGB) ===\")\n",
    "print(classification_report(y_test, y_pred_xgb))\n",
    "\n",
    "print(\"\\n=== MATRIZ DE CONFUSÃO (XGB) ===\")\n",
    "print(confusion_matrix(y_test, y_pred_xgb))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bc19c0",
   "metadata": {},
   "source": [
    "Recall da classe grave: ≈ 0,73\n",
    "\n",
    "F1-score da classe grave: ≈ 0,46\n",
    "\n",
    "Accuracy: ≈ 0,73\n",
    "\n",
    "Essa abordagem apresenta os resultados mais consistentes e robustos. A validação cruzada reduz a dependência de uma única divisão dos dados e confirma a superioridade do XGBoost para o problema analisado."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
